{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# 导入ground truth\n",
    "with open('ranked list truncation/data_prep/robust04_data/robust04_gt.pkl', 'rb') as f: gt = pickle.load(f)\n",
    "len(gt['301'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_F1(ranked_list: list, query: str, k: int, N_D: int) -> float:\n",
    "    \"\"\"\n",
    "    计算F1 score\n",
    "    k: 截断到第k个，从1计数\n",
    "    \"\"\"\n",
    "    count = sum([ranked_list[i] in gt[query] for i in range(k)])\n",
    "    p_k = count / k if k != 0 else 0\n",
    "    r_k = (count / N_D) if N_D != 0 else 0\n",
    "    return (2 * p_k * r_k / (p_k + r_k)) if p_k + r_k != 0 else 0\n",
    "\n",
    "\n",
    "def cal_DCG(ranked_list: list, query: str, k: int, N_D: int, penalty=-0.25, normalized=False) -> float:\n",
    "    \"\"\"\n",
    "    计算DCG\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    for i in range(k): value += (1 / math.log(i + 2, 2)) if ranked_list[i] in gt[query] else (penalty / math.log(i + 2, 2))\n",
    "    ideal_DCG = sum([1 / math.log(i + 2, 2) for i in range(k)]) if N_D >= k else sum([1 / math.log(i + 2, 2) for i in range(N_D)] + [penalty / math.log(i + 2, 2) for i in range(N_D, k)])\n",
    "    return value if not normalized else value / ideal_DCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixed-k的整套流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ori2rt(dataset_name: str, original_data: dict) -> dict:\n",
    "    rt_data = {}\n",
    "    for key in original_data: \n",
    "        rt_data[key] = [original_data[key]['retrieved_documents'][i]['doc_id'] for i in range(300)] if dataset_name == 'BM25' else [original_data[key][i]['doc_id'] for i in range(300)]\n",
    "    return rt_data\n",
    "\n",
    "\n",
    "def dataset_prepare(dataset_name: str) -> list:\n",
    "    # 这里不需要train\n",
    "    test_dataset_list = []\n",
    "    if dataset_name == 'BM25':\n",
    "        for i in range(1, 6):\n",
    "            with open('../data_prep/BM25_results/split_{}/BM25_test_s{}.pkl'.format(i, i), 'rb') as f: test_dataset_list.append(pickle.load(f))\n",
    "            test_dataset_list[-1] = ori2rt('BM25', test_dataset_list[-1])\n",
    "    else:\n",
    "        for i in range(1, 6):\n",
    "            with open('../data_prep/drmm_results/split_{}/drmm_test_s{}.pkl'.format(i, i), 'rb') as f: test_dataset_list.append(pickle.load(f))\n",
    "            test_dataset_list[-1] = ori2rt('DRMM', test_dataset_list[-1])\n",
    "    return test_dataset_list\n",
    "\n",
    "\n",
    "def test_scores(dataset: dict, gt: dict, k: list) -> float:\n",
    "    F1_test, DCG_test, NDCG_test = [], [], []\n",
    "    for key in dataset:\n",
    "        N_D = sum(dataset[key][i] in gt[key] for i in range(300))\n",
    "        F1_test.append(cal_F1(dataset[key], key, min(k[0] + 1, 300), N_D))\n",
    "        DCG_test.append(cal_DCG(dataset[key], key, min(k[1] + 1, 300), N_D))\n",
    "        NDCG_test.append(cal_DCG(dataset[key], key, min(k[2] + 1, 300), N_D, penalty=-0.78, normalized=True))\n",
    "    F1, DCG, NDCG = np.mean(F1_test), np.mean(DCG_test), np.mean(NDCG_test)\n",
    "    return F1, DCG, NDCG\n",
    "\n",
    "\n",
    "def k_fold(dataset_name: str, fixed_k=50) -> float:\n",
    "    test_dataset_list = dataset_prepare(dataset_name)\n",
    "    # 在测试集上得到对应于固定k的结果列表\n",
    "    F1_score_list, DCG_score_list, NDCG_score_list = [], [], []\n",
    "    for dataset in test_dataset_list:\n",
    "        results = test_scores(dataset, gt, [fixed_k - 1] * 3)\n",
    "        F1_score_list.append(results[0])\n",
    "        DCG_score_list.append(results[1])\n",
    "        NDCG_score_list.append(results[2])\n",
    "    return np.mean(F1_score_list), np.mean(DCG_score_list), np.mean(NDCG_score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25和DRMM的fixed-k结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 k = 5: (0.17474618534792546, 1.0747526914281167, 0.06382077566452259)\n",
      "BM25 k = 10: (0.24860544368215973, 1.489767906670021, -0.30712395236274276)\n",
      "BM25 k = 50: (0.2922332254375074, 1.7999268124946117, 2.9727567739747025)\n"
     ]
    }
   ],
   "source": [
    "print('BM25 k = 5: {}'.format(k_fold('BM25', 5)))\n",
    "print('BM25 k = 10: {}'.format(k_fold('BM25', 10)))\n",
    "print('BM25 k = 50: {}'.format(k_fold('BM25', 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRMM k = 5: (0.19574095035753317, 1.1092058369577242, 0.09973587877810637)\n",
      "DRMM k = 10: (0.26104689301570955, 1.5333460982847114, -0.02948479582395295)\n",
      "DRMM k = 50: (0.29725099683536305, 1.778329941465822, 3.044619810537594)\n"
     ]
    }
   ],
   "source": [
    "print('DRMM k = 5: {}'.format(k_fold('DRMM', 5)))\n",
    "print('DRMM k = 10: {}'.format(k_fold('DRMM', 10)))\n",
    "print('DRMM k = 50: {}'.format(k_fold('DRMM', 50)))"
   ]
  },
  {
   "source": [
    "## AAAI-2021 ranked_list and processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ori2rl(original_data: dict) -> dict:\n",
    "    rl_data = {}\n",
    "    for key in original_data: rl_data[key] = list(original_data[key].keys())\n",
    "    return rl_data\n",
    "\n",
    "\n",
    "def dataset_prepare(dataset_name: str) -> dict:\n",
    "    if dataset_name == 'BM25':\n",
    "        with open('ranked list truncation/data_prep/ranked_list_robust04/bm25_test.pkl', 'rb') as f: test_dataset = pickle.load(f)\n",
    "        test_ranked_list = ori2rl(test_dataset)\n",
    "    else:\n",
    "        with open('ranked list truncation/data_prep/ranked_list_robust04/drmm_test.pkl', 'rb') as f: test_dataset = pickle.load(f)\n",
    "        test_ranked_list = ori2rl(test_dataset)\n",
    "    return test_ranked_list\n",
    "\n",
    "\n",
    "def test_scores(dataset_name: str, gt: dict, k: int) -> tuple:\n",
    "    dataset = dataset_prepare(dataset_name)\n",
    "    F1_test, DCG_test = [], []\n",
    "    for key in dataset:\n",
    "        N_D = sum(dataset[key][i] in gt[key] for i in range(300))\n",
    "        F1_test.append(cal_F1(dataset[key], key, min(k, 300), N_D))\n",
    "        DCG_test.append(cal_DCG(dataset[key], key, min(k, 300), N_D))\n",
    "    F1, DCG = np.mean(F1_test), np.mean(DCG_test)\n",
    "    return F1, DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BM25 k = 5: (0.21689113823696973, 1.183738964566838)\nBM25 k = 10: (0.2767588393659435, 1.4833602514069963)\nBM25 k = 50: (0.2916372147309367, 1.3700313683419578)\n"
     ]
    }
   ],
   "source": [
    "print('BM25 k = 5: {}'.format(test_scores('BM25', gt, 5)))\n",
    "print('BM25 k = 10: {}'.format(test_scores('BM25', gt, 10)))\n",
    "print('BM25 k = 50: {}'.format(test_scores('BM25', gt, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DRMM k = 5: (0.19738517287426238, 1.2595384056546548)\nDRMM k = 10: (0.2619403550629597, 1.5774091770949645)\nDRMM k = 50: (0.29643726947393373, 1.6582145326505051)\n"
     ]
    }
   ],
   "source": [
    "print('DRMM k = 5: {}'.format(test_scores('DRMM', gt, 5)))\n",
    "print('DRMM k = 10: {}'.format(test_scores('DRMM', gt, 10)))\n",
    "print('DRMM k = 50: {}'.format(test_scores('DRMM', gt, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}