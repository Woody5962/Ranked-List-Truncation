{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入ground truth\n",
    "with open('../data_prep/robust04_data/robust04_gt.pkl', 'rb') as f: gt = pickle.load(f)\n",
    "len(gt['301'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_F1(ranked_list: list, query: str, k: int, N_D: int) -> float:\n",
    "    \"\"\"\n",
    "    计算F1 score\n",
    "    k: 截断到第k个，从1计数\n",
    "    \"\"\"\n",
    "    count = sum([ranked_list[i] in gt[query] for i in range(k)])\n",
    "    p_k = (count / k) if k != 0 else 0\n",
    "    r_k = (count / N_D) if N_D != 0 else 0\n",
    "    return (2 * p_k * r_k / (p_k + r_k)) if p_k + r_k != 0 else 0\n",
    "\n",
    "\n",
    "def cal_DCG(ranked_list: list, query: str, k: int, N_D: int, penalty=-0.25, normalized=False) -> float:\n",
    "    \"\"\"\n",
    "    计算DCG\n",
    "    \"\"\"\n",
    "    value = 0\n",
    "    for i in range(k): value += (1 / math.log(i + 2, 2)) if ranked_list[i] in gt[query] else (penalty / math.log(i + 2, 2))\n",
    "    ideal_DCG = sum([1 / math.log(i + 2, 2) for i in range(k)]) if N_D >= k else sum([1 / math.log(i + 2, 2) for i in range(N_D)] + [penalty / math.log(i + 2, 2) for i in range(N_D, k)])\n",
    "    return value if not normalized else value / ideal_DCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle的整体流程(自制数据集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ori2rt(dataset_name: str, original_data: dict) -> dict:\n",
    "    rt_data = {}\n",
    "    for key in original_data: \n",
    "        rt_data[key] = [original_data[key]['retrieved_documents'][i]['doc_id'] for i in range(300)] if dataset_name == 'BM25' else [original_data[key][i]['doc_id'] for i in range(300)]\n",
    "    return rt_data\n",
    "\n",
    "\n",
    "def dataset_prepare(dataset_name: str) -> list:\n",
    "    test_dataset_list = []\n",
    "    if dataset_name == 'BM25':\n",
    "        for i in range(1, 6):\n",
    "            with open('../data_prep/my_results/BM25_results/split_{}/BM25_test_s{}.pkl'.format(i, i), 'rb') as f: test_dataset_list.append(pickle.load(f))\n",
    "            test_dataset_list[-1] = ori2rt('BM25', test_dataset_list[-1])\n",
    "    else:\n",
    "        for i in range(1, 6):\n",
    "            with open('../data_prep/my_results/drmm_results/split_{}/drmm_test_s{}.pkl'.format(i, i), 'rb') as f: test_dataset_list.append(pickle.load(f))\n",
    "            test_dataset_list[-1] = ori2rt('DRMM', test_dataset_list[-1])\n",
    "    return test_dataset_list\n",
    "\n",
    "\n",
    "def test_scores(dataset: dict, gt: dict) -> tuple:\n",
    "    F1_test, DCG_test, NDCG_test = [], [], []\n",
    "    for key in dataset:\n",
    "        per_k_F1, per_k_DCG, N_D = [], [], sum(dataset[key][i] in gt[key] for i in range(300))\n",
    "        for i in range(300): \n",
    "            per_k_F1.append(cal_F1(dataset[key], key, i, N_D))\n",
    "            per_k_DCG.append(cal_DCG(dataset[key], key, i, N_D))\n",
    "        F1_test.append(max(per_k_F1))\n",
    "        DCG_test.append(max(per_k_DCG))\n",
    "    F1_result, DCG_result = np.mean(F1_test), np.mean(DCG_test)\n",
    "    return F1_result, DCG_result\n",
    "\n",
    "\n",
    "def k_fold(dataset_name: str) -> float:\n",
    "    test_dataset_list = dataset_prepare(dataset_name)\n",
    "    # 在测试集中得到先验指导的结果列表\n",
    "    F1_list, DCG_list = [], []\n",
    "    for dataset in test_dataset_list:\n",
    "        F1_result, DCG_result = test_scores(dataset, gt)\n",
    "        F1_list.append(F1_result)\n",
    "        DCG_list.append(DCG_result)\n",
    "    return np.mean(F1_list), np.mean(DCG_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAAI-2021 ranked_list and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ori2rl(original_data: dict) -> dict:\n",
    "    rl_data = {}\n",
    "    for key in original_data: rl_data[key] = list(original_data[key].keys())\n",
    "    return rl_data\n",
    "\n",
    "\n",
    "def dataset_prepare(dataset_name: str) -> dict:\n",
    "    if dataset_name == 'BM25':\n",
    "        with open('../data_prep/wc_results/bm25_test.pkl', 'rb') as f: test_dataset = pickle.load(f)\n",
    "        test_ranked_list = ori2rl(test_dataset)\n",
    "    else:\n",
    "        with open('../data_prep/wc_results/drmm_test.pkl', 'rb') as f: test_dataset = pickle.load(f)\n",
    "        test_ranked_list = ori2rl(test_dataset)\n",
    "    return test_ranked_list\n",
    "\n",
    "\n",
    "def test_scores(dataset_name: str, gt: dict) -> tuple:\n",
    "    dataset = dataset_prepare(dataset_name)\n",
    "    F1_test, DCG_test = [], []\n",
    "    for key in dataset:\n",
    "        per_k_F1, per_k_DCG, N_D = [], [], sum(dataset[key][i] in gt[key] for i in range(300))\n",
    "        for i in range(300): \n",
    "            per_k_F1.append(cal_F1(dataset[key], key, i, N_D))\n",
    "            per_k_DCG.append(cal_DCG(dataset[key], key, i, N_D))\n",
    "        F1_test.append(max(per_k_F1))\n",
    "        DCG_test.append(max(per_k_DCG))\n",
    "    F1_result, DCG_result = np.mean(F1_test), np.mean(DCG_test)\n",
    "    return F1_result, DCG_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经过召回优化的drmm-tks处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ori2rl(original_data: dict) -> dict:\n",
    "    rl_data = {}\n",
    "    for key in original_data: \n",
    "        rl_data[key] = [original_data[key][i]['doc_id'] for i in range(300)]\n",
    "    return rl_data\n",
    "\n",
    "\n",
    "def dataset_prepare() -> list:\n",
    "    test_dataset_list = []\n",
    "    for i in range(1, 6):\n",
    "        with open('../data_prep/my_results/drmm_tks_results/split_{}/drmm_tks_test_s{}.pkl'.format(i, i), 'rb') as f: test_dataset_list.append(pickle.load(f))\n",
    "        test_dataset_list[-1] = ori2rl(test_dataset_list[-1])\n",
    "    return test_dataset_list\n",
    "\n",
    "\n",
    "def test_scores(dataset: dict, gt: dict) -> tuple:\n",
    "    F1_test, DCG_test = [], []\n",
    "    for key in dataset:\n",
    "        per_k_F1, per_k_DCG, N_D = [], [], sum(dataset[key][i] in gt[key] for i in range(300))\n",
    "        for i in range(300): \n",
    "            per_k_F1.append(cal_F1(dataset[key], key, i, N_D))\n",
    "            per_k_DCG.append(cal_DCG(dataset[key], key, i, N_D))\n",
    "        F1_test.append(max(per_k_F1))\n",
    "        DCG_test.append(max(per_k_DCG))\n",
    "    F1_result, DCG_result = np.mean(F1_test), np.mean(DCG_test)\n",
    "    return F1_result, DCG_result\n",
    "\n",
    "\n",
    "def k_fold() -> float:\n",
    "    test_dataset_list = dataset_prepare()\n",
    "    # 在测试集中得到先验指导的结果列表\n",
    "    F1_list, DCG_list = [], []\n",
    "    for dataset in test_dataset_list:\n",
    "        F1_result, DCG_result = test_scores(dataset, gt)\n",
    "        F1_list.append(F1_result)\n",
    "        DCG_list.append(DCG_result)\n",
    "    return np.mean(F1_list), np.mean(DCG_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25和DRMM的Oracle结果(吴晨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Oracle: (0.44132237243494216, 2.7876816987782345)\n",
      "DRMM Oracle: (0.4398188071248245, 3.1463638415401878)\n"
     ]
    }
   ],
   "source": [
    "print('BM25 Oracle: {}'.format(test_scores('BM25', gt)))\n",
    "print('DRMM Oracle: {}'.format(test_scores('DRMM', gt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25和DRMM的Oracle结果(自制)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Oracle: (0.4294348991288629, 3.2136030192457157)\n",
      "DRMM Oracle: (0.45171301531736374, 3.1570649715650303)\n"
     ]
    }
   ],
   "source": [
    "print('BM25 Oracle: {}'.format(k_fold('BM25')))\n",
    "print('DRMM Oracle: {}'.format(k_fold('DRMM')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRMM-TKS Oracle结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRMM_TKS Oracle: (0.8392034355460855, 12.946841402517625)\n"
     ]
    }
   ],
   "source": [
    "print('DRMM_TKS Oracle: {}'.format(k_fold()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML] *",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
